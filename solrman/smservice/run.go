// Copyright 2016 FullStory, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package smservice

import (
	"fmt"
	"runtime"
	"sort"
	"strings"
	"time"

	"github.com/fullstorydev/gosolr/solrman/smmodel"
	"github.com/fullstorydev/gosolr/solrman/solrmanapi"
	"github.com/samuel/go-zookeeper/zk"
)

const (
	movesPerCycle                 = 10               // How many shards to move at a time
	iterationSleep                = 1 * time.Minute  // How long to sleep each attempt, no matter what
	quiescenceSleep               = 10 * time.Minute // How long to sleep when stability is reached
	splitsPerMachine              = 4                // How many shard splitting jobs to schedule on 1 physical machine at a time
	splitShardsWithDocCount       = 5000000          // Split shards with doc count > this
	allSplitsDocCountTrigger      = 5005000          // But don't do any splits until at least one shard > this
	allowedMinToMaxShardSizeRatio = 0.2              // Ratio of smallest shard to biggest shard < this then warn about imbalance
)

// Runs the main solr management loop, never returns.
func (s *SolrManService) RunSolrMan() {
	s.setStatusOp("solrman is starting up")

	first := true
	for {
		if !first {
			time.Sleep(iterationSleep)
		}
		first = false

		if s.ZooClient.State() != zk.StateHasSession {
			s.setStatusOp("not connected to zk")
			s.Logger.Warningf("not connected to zk")
			continue
		}

		if s.hasInProgressOps() {
			s.clearStatusOp()
			s.Logger.Debugf("in progress ops")
			continue
		}

		if disabled, err := func() (interface{}, error) {
			conn := s.RedisPool.Get()
			defer conn.Close()
			return conn.Do("GET", DisableRedisKey)
		}(); err != nil {
			s.setStatusOp("failed to query redis for disabled state")
			s.Logger.Errorf("failed to query redis for disabled state: %s", err)
			continue
		} else if disabled != nil {
			s.setStatusOp("solrman is disabled")
			s.Logger.Infof("solrman is disabled")
			continue
		}

		clusterState, err := s.ClusterState()
		if err != nil {
			s.setStatusOp("failed to retrieve cluster state")
			s.Logger.Errorf("failed to retrieve cluster state: %s", err)
			continue
		}

		liveNodes, err := s.SolrMonitor.GetLiveNodes()
		if err != nil {
			s.setStatusOp("failed to retrieve live nodes")
			s.Logger.Errorf("failed to retrieve live nodes: %s", err)
			continue
		}

		problems := listClusterProblems(s.Logger, clusterState, liveNodes)

		if len(problems) > 0 {
			s.Logger.Warningf("cluster state is not golden, trying to fix...")
			// TODO(scottb): disabled for now.
			// Try to goldenize it. Use Midas' finger or sorcerer's stone
			// solveClusterProblems(s.Logger, clusterState, s.solrClient, problems)

			// Remove any problem nodes from the model and operate on a reduced cluster
			for _, problem := range problems {
				node := problem.Node
				if _, ok := clusterState.SolrNodes[node]; ok {
					s.Logger.Warningf("removing bad node %s from model", node)
					delete(clusterState.SolrNodes, node)
				}
			}

			if len(clusterState.SolrNodes) < 1 {
				s.setStatusOp("no healthy nodes, skipping; see logs for details")
				continue
			}
		}

		badlyBalancedOrgs := flagBadlyBalancedOrgs(s.Logger, s, clusterState)
		if len(badlyBalancedOrgs) > 0 {
			s.Logger.Warningf(fmt.Sprintf("There are %d orgs with badly balanced shards.", len(badlyBalancedOrgs)))
		}

		shardSplits := computeShardSplits(s, clusterState)
		anySplits := false
		for _, shardSplit := range shardSplits {
			if _, ok := badlyBalancedOrgs[shardSplit.Collection]; ok {
				s.Logger.Warningf("skipping split for badly balanced org %s_%s", shardSplit.Collection, shardSplit.Shard)
				continue
			}

			s.Logger.Infof("Scheduling split operation %v", shardSplit)
			result, err := s.SplitShard(shardSplit)
			if err != nil {
				s.Logger.Errorf("failed to schedule autogenerated split %+v: %s", shardSplit, err)
			} else if result.Error != "" {
				s.Logger.Warningf("failed to schedule autogenerated split %+v: %s", shardSplit, result.Error)
			} else {
				s.Logger.Infof("scheduled autogenerated split %+v", shardSplit)
				anySplits = true
			}
		}

		if anySplits {
			continue
		}

		// Long-running computation!
		s.setStatusOp("computing shard moves")
		shardMoves, err := computeShardMoves(clusterState, movesPerCycle)
		if err != nil {
			s.setStatusOp("failed to compute shard moves")
			s.Logger.Errorf("failed to compute shard moves: %s", err)
			continue
		}

		if len(shardMoves) == 0 {
			// Sleep an extra long time if everything is gucci.
			s.setStatusOp("nothing to do, everything is well balanced!")
			s.Logger.Debugf("nothing to do, everything is well balanced!")
			time.Sleep(quiescenceSleep - iterationSleep)
			continue
		}

		// Double-check we're in a good state before queuing the moves.
		if s.ZooClient.State() != zk.StateHasSession {
			s.setStatusOp("not connected to zk")
			s.Logger.Warningf("not connected to zk")
			continue
		}

		if s.hasInProgressOps() {
			s.clearStatusOp()
			s.Logger.Debugf("in progress ops")
			continue
		}

		for _, shardMove := range shardMoves {
			result, err := s.MoveShard(shardMove)
			if err != nil {
				s.Logger.Errorf("failed to schedule autogenerated move %+v: %s", shardMove, err)
			} else if result.Error != "" {
				s.Logger.Warningf("failed to schedule autogenerated move %+v: %s", shardMove, result.Error)
			} else {
				s.Logger.Infof("scheduled autogenerated move %+v", shardMove)
			}
		}
	}
}

func (s *SolrManService) hasInProgressOps() bool {
	s.mu.Lock()
	defer s.mu.Unlock()
	return len(s.inProgressOps) > 0
}

// For admin visibility
func (s *SolrManService) clearStatusOp() {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.statusOp = nil
}

// For admin visibility
func (s *SolrManService) setStatusOp(status string) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.statusOp = &solrmanapi.OpRecord{
		StartedMs: nowMillis(),
		Operation: solrmanapi.OpStatus,
		Error:     status,
	}
}

const (
	ProblemInactive          = "inactive"
	ProblemConstruction      = "construction"
	ProblemNodedown          = "nodeDown"
	ProblemShardDown         = "down"
	ProblemNegativeNumDocs   = "negativeNumDocs"
	ProblemNegativeIndexSize = "negativeIndexSize"
)

type ClusterProblem struct {
	Node       string
	Collection string
	Shard      string
	Problem    string
}

func listClusterProblems(logger Logger, clusterState *solrmanapi.SolrmanStatusResponse, liveNodes []string) []*ClusterProblem {
	liveNodeSet := map[string]bool{}
	for _, liveNode := range liveNodes {
		liveNodeSet[liveNode] = true
	}
	problems := make([]*ClusterProblem, 0)

	for _, nodeStatus := range clusterState.SolrNodes {
		if !liveNodeSet[nodeStatus.NodeName] {
			logger.Warningf("%s not found in liveNodeSet", nodeStatus.NodeName)
			problems = append(problems, &ClusterProblem{nodeStatus.NodeName, "", "", ProblemNodedown})
			return problems
		}
		for _, coreStatus := range nodeStatus.Cores {
			if coreStatus.ShardState != "active" {
				logger.Debugf("%s:%s_%s shard in state: %s", coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard, coreStatus.ShardState)
				problems = append(problems, &ClusterProblem{coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard, coreStatus.ShardState})
			}
			if coreStatus.ReplicaState != "active" {
				logger.Debugf("%s:%s_%s replica in state %s", coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard, coreStatus.ReplicaState)
				problems = append(problems, &ClusterProblem{coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard, coreStatus.ReplicaState})
			}
			if coreStatus.IndexSize < 0 {
				logger.Debugf("%s:%s_%s negative index size", coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard)
				problems = append(problems, &ClusterProblem{coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard, ProblemNegativeIndexSize})
			}
			if coreStatus.NumDocs < 0 {
				logger.Debugf("%s:%s_%s negative doc count", coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard)
				problems = append(problems, &ClusterProblem{coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard, ProblemNegativeNumDocs})
			}
		}
	}
	return problems
}

func solveClusterProblems(logger Logger, clusterState *solrmanapi.SolrmanStatusResponse, solrc *SolrClient, problems []*ClusterProblem) {
	logger.Infof("Solving problems of cluster...")
	for _, p := range problems {
		switch p.Problem {
		case ProblemInactive, ProblemConstruction:
			// delete the shard
			if err := solrc.DeleteShard(p.Collection, p.Shard); err != nil {
				logger.Errorf("failed to delete shard %q of collection %q: %s", p.Shard, p.Collection, err)
			} else {
				logger.Debugf("deleted shard %q of collection %q", p.Shard, p.Collection)
			}
		case ProblemNodedown, ProblemNegativeIndexSize, ProblemNegativeNumDocs, ProblemShardDown:
			// nop | don't know | don't care | etc.
		}
	}
}

type SplitShardRequestWithSize struct {
	solrmanapi.SplitShardRequest
	NumDocs int64
}

type byNumDocsDesc []*SplitShardRequestWithSize

func (s byNumDocsDesc) Len() int {
	return len(s)
}
func (s byNumDocsDesc) Swap(i, j int) {
	s[i], s[j] = s[j], s[i]
}
func (s byNumDocsDesc) Less(i, j int) bool {
	return s[i].NumDocs > s[j].NumDocs
}

// MaxInt64 returns the maximum of int64 values.
func MaxInt64(a, b int64) int64 {
	if a > b {
		return a
	}
	return b
}

// MinInt64 returns the minimum of int64 values.
func MinInt64(a, b int64) int64 {
	if a < b {
		return a
	}
	return b
}

func flagBadlyBalancedOrgs(logger Logger, s *SolrManService, clusterState *solrmanapi.SolrmanStatusResponse) map[string]bool {
	orgToMin := make(map[string]int64)
	orgToMax := make(map[string]int64)

	getOrg := func(coreName string) string {
		return strings.Split(coreName, "_")[0]
	}

	for _, v := range clusterState.SolrNodes {
		for coreName, status := range v.Cores {
			org := getOrg(coreName)

			if min, ok := orgToMin[org]; ok {
				orgToMin[org] = MinInt64(min, status.NumDocs)
			} else {
				orgToMin[org] = status.NumDocs
			}

			if max, ok := orgToMax[org]; ok {
				orgToMax[org] = MaxInt64(max, status.NumDocs)
			} else {
				orgToMax[org] = status.NumDocs
			}
		}
	}

	badlyBalancedOrgs := make(map[string]bool)
	for org, max := range orgToMax {
		if (2+float64(orgToMin[org]))/(2+float64(max)) < allowedMinToMaxShardSizeRatio {
			badlyBalancedOrgs[org] = true
			logger.Warningf("Shards are getting imbalanced for org: " + org)
		}
	}

	return badlyBalancedOrgs
}

func computeShardSplits(s *SolrManService, clusterState *solrmanapi.SolrmanStatusResponse) []*solrmanapi.SplitShardRequest {
	machineToSplitOps := make(map[string][]*SplitShardRequestWithSize)
	anyShardTooBig := false
	for machine, v := range clusterState.SolrNodes {
		for _, status := range v.Cores {
			// Continue if this collection has too many shards i.e greater tha 8 * #solr machines
			if collstate, err := s.SolrMonitor.GetCollectionState(status.Collection); err != nil {
				continue
			} else if len(collstate.Shards) > 8*len(clusterState.SolrNodes) {
				continue
			}

			if status.NumDocs > allSplitsDocCountTrigger {
				anyShardTooBig = true
			}

			if status.NumDocs > splitShardsWithDocCount {
				x := &SplitShardRequestWithSize{
					solrmanapi.SplitShardRequest{
						status.Collection,
						status.Shard,
					},
					status.NumDocs,
				}
				machineToSplitOps[machine] = append(machineToSplitOps[machine], x)
			}
		}
	}

	if !anyShardTooBig {
		return nil
	}

	// sort by biggest to smallest shards
	// keep biggest splitsPerMachine shards only
	// flatten and return

	splitOps := make([]*solrmanapi.SplitShardRequest, 0)
	for _, ops := range machineToSplitOps {
		sort.Sort(byNumDocsDesc(ops))
		for i, op := range ops {
			if i == splitsPerMachine {
				break
			}
			splitOps = append(splitOps, &op.SplitShardRequest)
		}
	}

	return splitOps
}

func computeShardMoves(clusterState *solrmanapi.SolrmanStatusResponse, count int) ([]*solrmanapi.MoveShardRequest, error) {
	baseModel, err := createModel(clusterState)
	if err != nil {
		return nil, err
	}

	// Leave 1 cores open for handling queries / etc.
	numCPU := runtime.GOMAXPROCS(0) - 1
	if numCPU < 1 {
		numCPU = 1
	}

	var moves []*smmodel.Move
	immobileCores := map[string]bool{}
	m := baseModel
	for i := 0; i < count; i++ {
		mPrime, move := m.ComputeNextMove(numCPU, immobileCores)
		if m == mPrime {
			break
		}
		moves = append(moves, move)
		immobileCores[move.Core.Name] = true
		m = mPrime
	}

	var shardMoves []*solrmanapi.MoveShardRequest
	if len(moves) > 0 {
		shardMoves = make([]*solrmanapi.MoveShardRequest, len(moves))
		for i, m := range moves {
			shardMoves[i] = &solrmanapi.MoveShardRequest{
				Collection: m.Core.Collection,
				Shard:      m.Core.Shard,
				SrcNode:    m.FromNode.Address,
				DstNode:    m.ToNode.Address,
			}
		}
	}

	return shardMoves, nil
}

func createModel(clusterState *solrmanapi.SolrmanStatusResponse) (*smmodel.Model, error) {
	var currentNode *smmodel.Node
	m := &smmodel.Model{}
	seenNodeNames := map[string]bool{}
	collectionMap := make(map[string]*smmodel.Collection)

	for _, nodeStatus := range clusterState.SolrNodes {
		currentNode = &smmodel.Node{Name: nodeStatus.Hostname, Address: nodeStatus.NodeName}
		if seenNodeNames[nodeStatus.NodeName] {
			return nil, errorf("already seen: %v", nodeStatus.NodeName)
		}
		seenNodeNames[nodeStatus.NodeName] = true
		m.Nodes = append(m.Nodes, currentNode)

		for _, coreStatus := range nodeStatus.Cores {
			collName := coreStatus.Collection
			collection := collectionMap[collName]
			if collection == nil {
				collection = &smmodel.Collection{Name: collName}
				collectionMap[collName] = collection
				m.Collections = append(m.Collections, collection)
			}

			core := &smmodel.Core{
				Name:       coreStatus.Name,
				Collection: collName,
				Shard:      coreStatus.Shard,
				Docs:       float64(coreStatus.NumDocs),
				Size:       float64(coreStatus.IndexSize),
			}
			collection.Add(core)
			currentNode.Add(core)
			m.Add(core)
		}
	}
	return m, nil
}
