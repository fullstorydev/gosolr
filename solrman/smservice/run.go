// Copyright 2016 FullStory, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package smservice

import (
	"runtime"
	"time"

	"github.com/fullstorydev/gosolr/solrman/smmodel"
	"github.com/fullstorydev/gosolr/solrman/solrmanapi"
	"github.com/samuel/go-zookeeper/zk"
)

const (
	movesPerCycle        = 10               // How many shards to move at a time
	iterationSleep       = 1 * time.Minute  // How long to sleep each attempt, no matter what
	quiescenceSleep      = 10 * time.Minute // How long to sleep when stability is reached
	splitsPerMachine     = 4                // How many shard splitting jobs to schedule on 1 physical machine at a time
	maxAllowedDocs       = 5e6              // Max allowed docs allowed on a shard (5M)
	maxAllowedExcessDocs = 1e6
)

// Runs the main solr management loop, never returns.
func (s *SolrManService) RunSolrMan() {
	s.setStatusOp("solrman is starting up")

	first := true
	for {
		if !first {
			time.Sleep(iterationSleep)
		}
		first = false

		if s.ZooClient.State() != zk.StateHasSession {
			s.setStatusOp("not connected to zk")
			s.Logger.Warningf("not connected to zk")
			continue
		}

		if s.hasInProgressOps() {
			s.clearStatusOp()
			s.Logger.Debugf("in progress ops")
			continue
		}

		if disabled, err := func() (interface{}, error) {
			conn := s.RedisPool.Get()
			defer conn.Close()
			return conn.Do("GET", DisableRedisKey)
		}(); err != nil {
			s.setStatusOp("failed to query redis for disabled state")
			s.Logger.Errorf("failed to query redis for disabled state: %s", err)
			continue
		} else if disabled != nil {
			s.setStatusOp("solrman is disabled")
			s.Logger.Infof("solrman is disabled")
			continue
		}

		clusterState, err := s.ClusterState()
		if err != nil {
			s.setStatusOp("failed to retrieve cluster state")
			s.Logger.Errorf("failed to retrieve cluster state: %s", err)
			continue
		}

		liveNodes, err := s.SolrMonitor.GetLiveNodes()
		if err != nil {
			s.setStatusOp("failed to retrieve live nodes")
			s.Logger.Errorf("failed to retrieve live nodes: %s", err)
			continue
		}

		if !clusterStateGolden(s.Logger, clusterState, liveNodes) {
			s.setStatusOp("cluster state is not golden, skipping; see logs for details")
			s.Logger.Warningf("cluster state is not golden, skipping")
			continue
		}

		shardSplits := computeShardSplits(s, clusterState)

		for _, shardSplit := range shardSplits {
			s.Logger.Infof("Scheduling split operation %v", shardSplit)
			result, err := s.SplitShard(shardSplit)
			if err != nil {
				s.Logger.Errorf("failed to schedule autogenerated split %+v: %s", shardSplit, err)
			} else if result.Error != "" {
				s.Logger.Warningf("failed to schedule autogenerated split %+v: %s", shardSplit, result.Error)
			} else {
				s.Logger.Infof("scheduled autogenerated split %+v", shardSplit)
			}
		}

		if len(shardSplits) > 0 {
			continue
		}

		// Long-running computation!
		s.setStatusOp("computing shard moves")
		shardMoves, err := computeShardMoves(clusterState, movesPerCycle)
		if err != nil {
			s.setStatusOp("failed to compute shard moves")
			s.Logger.Errorf("failed to compute shard moves: %s", err)
			continue
		}

		if len(shardMoves) == 0 {
			// Sleep an extra long time if everything is gucci.
			s.setStatusOp("nothing to do, everything is well balanced!")
			s.Logger.Debugf("nothing to do, everything is well balanced!")
			time.Sleep(quiescenceSleep - iterationSleep)
			continue
		}

		// Double-check we're in a good state before queuing the moves.
		if s.ZooClient.State() != zk.StateHasSession {
			s.setStatusOp("not connected to zk")
			s.Logger.Warningf("not connected to zk")
			continue
		}

		if s.hasInProgressOps() {
			s.clearStatusOp()
			s.Logger.Debugf("in progress ops")
			continue
		}

		for _, shardMove := range shardMoves {
			result, err := s.MoveShard(shardMove)
			if err != nil {
				s.Logger.Errorf("failed to schedule autogenerated move %+v: %s", shardMove, err)
			} else if result.Error != "" {
				s.Logger.Warningf("failed to schedule autogenerated move %+v: %s", shardMove, result.Error)
			} else {
				s.Logger.Infof("scheduled autogenerated move %+v", shardMove)
			}
		}
	}
}

func (s *SolrManService) hasInProgressOps() bool {
	s.mu.Lock()
	defer s.mu.Unlock()
	return len(s.inProgressOps) > 0
}

// For admin visibility
func (s *SolrManService) clearStatusOp() {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.statusOp = nil
}

// For admin visibility
func (s *SolrManService) setStatusOp(status string) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.statusOp = &solrmanapi.OpRecord{
		StartedMs: nowMillis(),
		Operation: solrmanapi.OpStatus,
		Error:     status,
	}
}

// Don't bother running solrman if anything is currently down, the results will be wrong.
func clusterStateGolden(logger Logger, clusterState *solrmanapi.SolrmanStatusResponse, liveNodes []string) bool {
	liveNodeSet := map[string]bool{}
	for _, liveNode := range liveNodes {
		liveNodeSet[liveNode] = true
	}

	result := true
	for _, nodeStatus := range clusterState.SolrNodes {
		if !liveNodeSet[nodeStatus.NodeName] {
			logger.Warningf("%s not found in liveNodeSet", nodeStatus.NodeName)
			return false
		}
		for _, coreStatus := range nodeStatus.Cores {
			if coreStatus.ShardState != "active" {
				logger.Debugf("%s:%s_%s shard in state: %s", coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard, coreStatus.ShardState)
				result = false
			}
			if coreStatus.ReplicaState != "active" {
				logger.Debugf("%s:%s_%s replica in state %s", coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard, coreStatus.ReplicaState)
				result = false
			}
			if coreStatus.IndexSize < 0 {
				logger.Debugf("%s:%s_%s negative index size", coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard)
				result = false
			}
			if coreStatus.NumDocs < 0 {
				logger.Debugf("%s:%s_%s negative doc count", coreStatus.NodeName, coreStatus.Collection, coreStatus.Shard)
				result = false
			}
		}
	}
	return result
}

func computeShardSplits(s *SolrManService, clusterState *solrmanapi.SolrmanStatusResponse) []*solrmanapi.SplitShardRequest {
	splitOps := make([]*solrmanapi.SplitShardRequest, 0)

	// We allow shards to grow bigger than maxAllowedShardBytes
	// but start splitting shards that bigger than maxAllowedShardBytes
	// if any shard gets bigger than maxAllowedShardBytes + maxAllowedExcessShardBytes
	anyShardTooBig := false

	for _, v := range clusterState.SolrNodes {
		nSplitsThisMachine := 0 // # split ops scheduled on this node
		for _, status := range v.Cores {
			// Continue if this collection has too many shards i.e greater tha 8 * #solr machines
			if collstate, err := s.SolrMonitor.GetCollectionState(status.Collection); err != nil {
				continue
			} else if len(collstate.Shards) > 8*len(clusterState.SolrNodes) {
				continue
			}

			if status.NumDocs > maxAllowedDocs+maxAllowedExcessDocs {
				anyShardTooBig = true
			}

			if nSplitsThisMachine >= splitsPerMachine {
				continue
			}

			if status.NumDocs > maxAllowedDocs {
				x := &solrmanapi.SplitShardRequest{
					Collection: status.Collection,
					Shard:      status.Shard,
				}
				splitOps = append(splitOps, x)
				nSplitsThisMachine++
			}
		}
	}

	if !anyShardTooBig {
		return make([]*solrmanapi.SplitShardRequest, 0)
	}

	return splitOps
}

func computeShardMoves(clusterState *solrmanapi.SolrmanStatusResponse, count int) ([]*solrmanapi.MoveShardRequest, error) {
	baseModel, err := createModel(clusterState)
	if err != nil {
		return nil, err
	}

	// Leave 2 cores open for handling queries / etc.
	numCPU := runtime.GOMAXPROCS(0) - 2
	if numCPU < 1 {
		numCPU = 1
	}

	var moves []*smmodel.Move
	immobileCores := map[string]bool{}
	m := baseModel
	for i := 0; i < count; i++ {
		mPrime, move := m.ComputeNextMove(numCPU, immobileCores)
		if m == mPrime {
			break
		}
		moves = append(moves, move)
		immobileCores[move.Core.Name] = true
		m = mPrime
	}

	var shardMoves []*solrmanapi.MoveShardRequest
	if len(moves) > 0 {
		shardMoves = make([]*solrmanapi.MoveShardRequest, len(moves))
		for i, m := range moves {
			shardMoves[i] = &solrmanapi.MoveShardRequest{
				Collection: m.Core.Collection,
				Shard:      m.Core.Shard,
				SrcNode:    m.FromNode.Address,
				DstNode:    m.ToNode.Address,
			}
		}
	}

	return shardMoves, nil
}

func createModel(clusterState *solrmanapi.SolrmanStatusResponse) (*smmodel.Model, error) {
	var currentNode *smmodel.Node
	m := &smmodel.Model{}
	seenNodeNames := map[string]bool{}
	collectionMap := make(map[string]*smmodel.Collection)

	for _, nodeStatus := range clusterState.SolrNodes {
		currentNode = &smmodel.Node{Name: nodeStatus.Hostname, Address: nodeStatus.NodeName}
		if seenNodeNames[nodeStatus.NodeName] {
			return nil, errorf("already seen: %v", nodeStatus.NodeName)
		}
		seenNodeNames[nodeStatus.NodeName] = true
		m.Nodes = append(m.Nodes, currentNode)

		for _, coreStatus := range nodeStatus.Cores {
			collName := coreStatus.Collection
			collection := collectionMap[collName]
			if collection == nil {
				collection = &smmodel.Collection{Name: collName}
				collectionMap[collName] = collection
				m.Collections = append(m.Collections, collection)
			}

			core := &smmodel.Core{
				Name:       coreStatus.Name,
				Collection: collName,
				Shard:      coreStatus.Shard,
				Docs:       float64(coreStatus.NumDocs),
				Size:       float64(coreStatus.IndexSize),
			}
			collection.Add(core)
			currentNode.Add(core)
			m.Add(core)
		}
	}
	return m, nil
}
